{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gurlon Docs","text":""},{"location":"#overview","title":"Overview","text":"<p><code>gurlon</code> is a library designed to make the process of exporting data from Dynamo to your local filesystem easier.</p> <p>Key Concepts</p> <p>There are 3 main steps to the <code>gurlon</code> export process:</p> <ol> <li>Instantiate a new <code>DataExporter</code> and invoke <code>export_data</code> to begin a DynamoDB PointInTimeExport to S3</li> <li>Call the <code>DataExporter</code> function <code>download_data</code> once the DynamoDB export is complete to combine the exported data into a single json file on your local filesystem</li> <li>Transform your local copy of the exported table data into another storage format: <code>csv</code>, <code>parquet</code></li> </ol>"},{"location":"#installation","title":"Installation","text":"pipuv <pre><code>pip install gurlon\n</code></pre> <pre><code>uv add gurlon\n</code></pre>"},{"location":"#export-data-from-dynamodb-to-s3","title":"Export Data from DynamoDB to S3","text":"<p>In order to eventually run SQL queries on your DynamoDB table data, it first needs to be exported to S3.</p> <p>PITR Must be Enabled</p> <p>Your DynamoDB table needs to have point-in-time recovery enabled in order to perform ExportTableToPointInTime operations.</p>"},{"location":"#create-a-dataexporter","title":"Create a <code>DataExporter</code>","text":"<p>Import the <code>DataExporter</code> class into your Python file, and create a <code>DataExporter</code> instance by passing the following parameters:</p> <ul> <li><code>aws_region: str</code></li> <li><code>table_name: str</code></li> <li><code>bucket_name: str</code></li> </ul> <pre><code>from gurlon.processor import DataExporter\n\nexporter = DataExporter(\"us-west-1\", \"gurlon-table\", \"gurlon-bucket\")\n</code></pre>"},{"location":"#provide-aws-credentials","title":"Provide AWS Credentials","text":"<p>Make sure the environment this code is executing in supplies your AWS credentials through either:</p> <ul> <li>Environment variables - AWS Docs Reference</li> <li>The <code>~/.aws/config</code> file - AWS Docs Reference</li> </ul> Additional Details on Authentication Process <p>Gurlon uses <code>boto3</code> to perform AWS operations, so you can read up more on the underlying authentication process here.</p>"},{"location":"#trigger-the-export","title":"Trigger the Export","text":"<p>Call the <code>export_data</code> function to begin exporting your table data to S3.</p> <p>If the operation succeeds, the export ARN will be returned.</p> <pre><code>from gurlon.processor import DataExporter\n\nexporter = DataExporter(\"us-west-1\", \"gurlon-table\", \"gurlon-bucket\")\nexport_arn = exporter.export_data()\n</code></pre>"},{"location":"#download-exported-data","title":"Download Exported Data","text":"<p>Once your table export to S3 is complete, you can download the data to your local filesystem.</p> <pre><code>from pathlib import Path\n\nfrom gurlon.processor import DataExporter\n\nexporter = DataExporter(\"us-west-1\", \"gurlon-table\", \"gurlon-bucket\")\nexporter.table_export_arn = \"YOUR:TABLE:EXPORT:ARN\"\n\ndownload_dir = Path.home() / \"Downloads\" / \"dynamodb_exports\"\nexporter.download_data(download_dir=download_dir)\n</code></pre>"},{"location":"#output","title":"Output","text":"<p>Gurlon takes care of decompressing the exported data and combining it into a valid JSON file. This combined JSON file is stored inside the <code>download_dir</code> you specified previously.</p>"},{"location":"#transform-the-data-to-different-file-types","title":"Transform the Data to Different File Types","text":"<p>Success</p> <p>Now that the exported data is present locally, you can begin to transform it into different formats.</p>"},{"location":"#create-a-datatransformer","title":"Create a <code>DataTransformer</code>","text":"<pre><code>from pathlib import Path\n\nfrom gurlon.processor import DataTransformer\n\ndownload_dir = Path.home() / \"Downloads\" / \"dynamodb_exports\"\ncombined_data = download_dir / \"combined_data.json\"\ntransformer = DataTransformer(combined_data)\n</code></pre>"},{"location":"#parquet","title":"Parquet","text":"<pre><code>from pathlib import Path\n\nfrom gurlon.processor import DataTransformer\n\ndownload_dir = Path.home() / \"Downloads\" / \"dynamodb_exports\"\ncombined_data = download_dir / \"combined_data.json\"\ntransformer = DataTransformer(combined_data)\n\nparquet = transformer.to_parquet()\n</code></pre>"},{"location":"#csv","title":"CSV","text":"<pre><code>from pathlib import Path\n\nfrom gurlon.processor import DataTransformer\n\ndownload_dir = Path.home() / \"Downloads\" / \"dynamodb_exports\"\ncombined_data = download_dir / \"combined_data.json\"\ntransformer = DataTransformer(combined_data)\n\ncsv = transformer.to_csv()\n</code></pre>"},{"location":"#duckdb","title":"DuckDB","text":"<pre><code>from pathlib import Path\n\nfrom gurlon.processor import DataTransformer\n\ndownload_dir = Path.home() / \"Downloads\" / \"dynamodb_exports\"\ncombined_data = download_dir / \"combined_data.json\"\ntransformer = DataTransformer(combined_data)\n\nduckdb = transformer.to_duckdb()\n</code></pre>"},{"location":"#sqlite-table","title":"SQLite Table","text":"<pre><code>from sqlmodel import Field, SQLModel\n\nfrom gurlon.processor import DataTransformer\n\n\nclass TableItemModel(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    user_id: str\n    user_name: str\n    email: str\n    role: str\n    full_name: str\n\ntransformer = DataTransformer(combined_data)\nsql = transformer.to_sqlmodel(TableItemModel)\n</code></pre>"}]}